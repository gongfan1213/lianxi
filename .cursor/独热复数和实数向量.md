我来详细为您讲解这些向量表示的概念：

## 1. 独热向量（One-Hot Vector）

### 基本概念
独热向量是一种**稀疏向量表示**，其中只有一个元素为1，其余都为0。

### 特点
- **维度等于词汇表大小**：如果有10000个词，向量就是10000维
- **稀疏性**：大部分元素为0，只有一个元素为1
- **正交性**：不同词的独热向量相互正交（内积为0）

### 示例
```python
# 假设词汇表：["猫", "狗", "鸟", "鱼"]
猫的独热向量：[1, 0, 0, 0]
狗的独热向量：[0, 1, 0, 0]
鸟的独热向量：[0, 0, 1, 0]
鱼的独热向量：[0, 0, 0, 1]
```

### 优缺点
**优点：**
- 简单直观，容易理解
- 每个词都有唯一表示

**缺点：**
- 维度灾难：词汇表大时向量维度极高
- 无法表达语义关系：所有词向量距离相等
- 稀疏性导致计算效率低

---

## 2. 实数向量（Real Vector）

### 基本概念
实数向量是**密集向量表示**，每个元素都是实数，通常维度远小于词汇表大小。

### 特点
- **低维密集**：通常100-1000维，远小于词汇表大小
- **语义丰富**：可以表达词汇间的语义关系
- **连续空间**：语义相似的词在向量空间中距离较近

### 示例
```python
# Word2Vec或GloVe生成的词向量（简化版）
猫的向量：[0.2, -0.5, 0.8, 0.1, -0.3]
狗的向量：[0.3, -0.4, 0.7, 0.2, -0.2]
鸟的向量：[-0.1, 0.6, -0.2, 0.9, 0.4]
鱼的向量：[-0.2, 0.5, -0.3, 0.8, 0.3]
```

### 优缺点
**优点：**
- 维度可控，计算效率高
- 能表达语义相似性
- 支持向量运算（加法、减法等）

**缺点：**
- 需要训练学习
- 可能丢失一些信息

---

## 3. 复数向量（Complex Vector）

### 基本概念
复数向量是**复数域上的向量表示**，每个元素都是复数（实部+虚部）。

### 特点
- **信息密度更高**：每个元素包含实部和虚部两个信息
- **相位信息**：可以编码更复杂的语义关系
- **数学性质丰富**：支持复数运算

### 示例
```python
# 复数向量表示（实部+虚部）
猫的复数向量：[0.2+0.1j, -0.5+0.3j, 0.8-0.2j]
狗的复数向量：[0.3+0.2j, -0.4+0.1j, 0.7-0.1j]
```

### 优缺点
**优点：**
- 信息容量更大
- 可以编码更复杂的语义关系
- 数学性质丰富

**缺点：**
- 计算复杂度高
- 训练困难
- 在NLP中应用较少

---

## 4. 三种向量的比较

| 特征 | 独热向量 | 实数向量 | 复数向量 |
|------|----------|----------|----------|
| **维度** | 词汇表大小 | 100-1000维 | 50-500维 |
| **稀疏性** | 极高稀疏 | 密集 | 密集 |
| **语义表达** | 无 | 强 | 更强 |
| **计算效率** | 低 | 高 | 中等 |
| **训练难度** | 无需训练 | 中等 | 高 |
| **应用场景** | 传统NLP | 现代NLP | 研究阶段 |

---

## 5. 实际应用中的选择

### 为什么NLP主要使用实数向量？

1. **平衡性**：在表达能力和计算效率之间取得平衡
2. **成熟度**：Word2Vec、GloVe、BERT等模型都使用实数向量
3. **实用性**：能够有效表达语义关系，支持向量运算
4. **稳定性**：训练稳定，效果可预测

### 独热向量的应用场景
- 简单的分类任务
- 特征工程中的类别编码
- 传统机器学习模型

### 复数向量的研究价值
- 探索更丰富的语义表示
- 研究相位信息的作用
- 未来可能的发展方向

---

## 6. 代码示例

```python
import numpy as np

# 独热向量
def create_one_hot(word, vocab):
    vector = np.zeros(len(vocab))
    vector[vocab.index(word)] = 1
    return vector

# 实数向量（模拟Word2Vec）
def create_real_vector(word, embedding_dim=100):
    # 实际应用中，这些向量是通过训练得到的
    return np.random.normal(0, 1, embedding_dim)

# 复数向量
def create_complex_vector(word, embedding_dim=50):
    real_part = np.random.normal(0, 1, embedding_dim)
    imag_part = np.random.normal(0, 1, embedding_dim)
    return real_part + 1j * imag_part

# 使用示例
vocab = ["猫", "狗", "鸟", "鱼"]
print("独热向量:", create_one_hot("猫", vocab))
print("实数向量:", create_real_vector("猫")[:5])  # 显示前5维
print("复数向量:", create_complex_vector("猫")[:3])  # 显示前3维
```

这就是三种向量表示的核心概念和区别。在现代NLP中，实数向量（如Word2Vec、GloVe、BERT embeddings）是最主流的选择，因为它们既有效又实用。